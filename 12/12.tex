\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{../common}
\usepackage{../pagesetup}
\usepackage{tikz}
% DEFINE COMMANDS IN PAGESETUP, NOT HERE

\begin{document}

\lecture{12}{October 16}{Sasha Rush}{Alexander Goldberg, Daniel Eaton, George Han, Moritz Graule, Kristo Ment}{Recurrent Neural Networks}

\subsection{Introduction}
Recall the basic structure of a time series model:
\smallskip

\begin{center}
	\begin{tikzpicture}
		\node[latent, scale=1.15]                                  (y1) {$y_{1}$};
		\node[latent, right=of y1, scale=1.15]                     (y2) {$y_{2}$};
		\node[latent, right=of y2, scale=1.15]                     (y3) {$y_{3}$};
		\node[latent, right=of y3]                                   (yt1) {$y_{t-1}$};
		\node[latent, right=of yt1, scale=1.15]                     (yt) {$y_{t}$};


		\edge [-] {y1} {y2} ;
		\edge [-] {y2} {y3} ;
 		\edge [-] {yt1} {yt} ;

		\path (y3) -- node[auto=false]{\dots} (yt1);

	\end{tikzpicture}
\end{center}

In previous lectures we discussed interpreting such a model as a UGM, with log-potentials given by:
$$\theta(y_{t}) + \theta(y_{t-1},y_{t})$$

What did we gain from this abstraction? Because all of these models have the same comditional independence structure, we are able to run the same algorithm for inference on all of these parameterizations (sum-product). Thus, conditioned on observed data, we may find the exact marginals: $p(y_{s}=v)$
\smallskip

Perhaps these structures are not necessary? Is exact inference required in cases where there is alot of data?

\subsection{RNNs}
\subsubsection{What is an RNN?}
Recall our discussion of using neural networks for classification. The UGM describing this setting is as follows:

\begin{center}
	\begin{tikzpicture}
		\node[latent, scale=1.15]                                  (y) {$y$};
		\node[latent, below=of y, xshift=-1cm, scale=1.15]      (x1) {$x_{1}$};
		\node[latent, below=of y, xshift=1cm, scale=1.15]       (xt) {$x_{T}$};

		\edge [-] {y} {x1} ;
		\edge [-] {y} {xt} ;
		\path (x1) -- node[auto=false]{\dots} (xt);

	\end{tikzpicture}
\end{center}

We could then choose to parameterize $p(y|x_{1:T})$ as a neural network:
$$p(y|x_{1:T}) = Softmax(\bold{w}^{T} \phi(x_{1:T} ; \theta))$$

where $\phi$ was just some linear combination of $x_{1:T}$ passed through a link function. If we wish to apply this scheme to cases in which $x_{1:T}$ is a \textit{sequence} we might think to use a $\phi$ of the following form:
$$\phi(x_{1:T} ; \theta) = tanh(\bold{w}\bold{x}) = tanh(\sum_{t=1}^{T} w^{(t)}x_{t})$$

However, the problem with this type of approach is that it is "time invarient," that is the same weights are shared by all of the $x_{t}$. To see why this is problematic, consider using a bag of words representation for the $X_{t}$ and encountering the two sentences: "The man ate the hot dog." and "The hot dog ate the man." While these two sentences are saying completely different things, they result in the same value generated by $\phi$.

Recurrent neural networks get around this problem by implementing the following choice of $\phi$:
$$\phi(x_{1:T} ; \theta) = tanh(\bold{w}^{(1)} x_{t} + \bold{w}^{(2)} \phi (x_{1:t-1}; \theta) + b)$$ 
where $\bold{w}^{(1)} x_{t}$ incorperates the current positional input, $\bold{w}^{(2)} \phi (x_{1:t-1}; \theta)$ carries information from the previous inputs, $b$ is the bias and $tanh$ is the chosen nonlinear transformation.

\smallskip

Representing this RNN as a computational graph:

\begin{center}
	\begin{tikzpicture}
		\node[latent, scale=0.5, label={\Large $h_{0}$}]                                  (h01) {};
		\node[latent, scale=0.5, below=of h01]                    (h02) {};
		\node[latent, scale=0.5, below=of h02]                    (h03) {};
		\node[latent, scale=0.5, right=of h01, xshift=3cm,label={\Large $h_{1}$}]                     (h11) {};
		\node[latent, scale=0.5, below=of h11]                    (h12) {};
		\node[latent, scale=0.5, below=of h12]                    (h13) {};
		\node[latent, scale=0.5, right=of h11, xshift=3cm, label={\Large $h_{2}$},label=right:{\Large{\dots}}]                     (h21) {};
		\node[latent, scale=0.5, below=of h21 ,label=right:{\Large{\dots}}]                    (h22) {};
		\node[latent, scale=0.5, below=of h22,label=right:{\Large{\dots}}]                    (h23) {};
		
		\node[latent, scale=0.5, below=of h03, xshift=1.5cm, yshift=1cm]        (x01) {};
		\node[latent, scale=0.5, right=of x01, xshift=-1.5cm, label=below:{\Large $x_{1}$}]        (x02) {};
		\node[latent, scale=0.5, right=of x02, xshift=-1.5cm]        (x03) {};
		\node[latent, scale=0.5, below=of h13, xshift=1.5cm, yshift=1cm]        (x11) {};
		\node[latent, scale=0.5, right=of x11, xshift=-1.5cm, label=below:{\Large $x_{2}$}]        (x12) {};
		\node[latent, scale=0.5, right=of x12, xshift=-1.5cm]        (x13) {};
		
		
		\edge [-] {h01} {h11,h12,h13} ;
		\edge [-] {h02} {h11,h12,h13} ;
		\edge [-] {h03} {h11,h12,h13} ;
		\edge [-] {h11} {h21,h22,h23} ;
		\edge [-] {h12} {h21,h22,h23} ;
		\edge [-] {h13} {h21,h22,h23} ;
		
		\edge [-] {x01} {h11,h12,h13} ;
		\edge [-] {x02} {h11,h12,h13} ;
		\edge [-] {x03} {h11,h12,h13} ;
		\edge [-] {x11} {h21,h22,h23} ;
		\edge [-] {x12} {h21,h22,h23} ;
		\edge [-] {x13} {h21,h22,h23} ;
% 		\edge [-] {y} {xt} ;
% 		\path (x1) -- node[auto=false]{\dots} (xt);

	\end{tikzpicture}
\end{center}

where we call $h_{t}$ the RNN hidden state. As you can see, each $h_{t}$ is a nonlinear function of $x_{1:t}$.

\subsubsection{RNN Training}

To understand how backpropagation works in RNNs, consider the functions that the NN is composed of:
$$h_{1} = tanh(\bold{w}^{(1)}\bold{x}_{1} + \bold{w}^{(2)}\bold{h}_{0} + b)$$
$$h_{2} = tanh(\bold{w}^{(1)}\bold{x}_{2} + \bold{w}^{(2)}\bold{h}_{1} + b)$$
$$\dots$$
$$h_{t} = tanh(\bold{w}^{(1)}\bold{x}_{t} + \bold{w}^{(2)}\bold{h}_{t-1} + b)$$

Notice that the only parameters to optimize in these expressions are $\bold{w}^{(1)}$ and $\bold{w}^{(2)}$, which are shared among all of the equations. In our normal representation of backpropagation we have:


\begin{center}
	\begin{tikzpicture}
	    \node[latent]                                          (box1) {};
		\node[below = of box1]                 (x1) {$x_{1}$};
		\node[right = of box1,xshift=-0.75cm,yshift=0.6cm]                 (h1) {$h_{1}$};
		\node[latent, right = of box1]                            (box2) {};
		\node[below = of box2]                 (x2) {$x_{2}$};
		\node[right = of box2,xshift=-0.75cm,yshift=0.6cm]                 (h2) {$h_{2}$};
		\node[latent, right = of box2]                            (box3) {};
		\node[below = of box3]                 (x3) {$x_{3}$};
		\node[right = of box3, xshift=-0.9cm]                 (dot) {\Large \dots};
		\node[latent, right = of box3]                            (boxt) {};
		\node[below = of boxt]                 (xt) {$x_{t}$};
		\node[right = of boxt,xshift=-0.75cm,yshift=0.6cm]                 (ht) {$h_{t}$};
		\node[right = of boxt]                            (loss) {loss};

    	\edge{x1}{box1}
    	\edge{box1}{box2}
    	\edge{x2}{box2}
    	\edge{box2}{box3}
    	\edge{x3}{box3}
    	\edge{xt}{boxt}
    	\edge{boxt}{loss}
        
        
	\end{tikzpicture}
\end{center}

Thus the size of the computational graph and the amount of backpropagation necessary will scale with the length of the input, T. Now, thinking of this situation like a simple feed-forward NN, how many layers does this network have?

\subsubsection{Issues: Network Layers}

This network will have $T$ layers, where $T$ is the length of the input, which may be very long. Thus, when performing back-propagation it is very likely that there will be problems of gradient instability -- very high (exploding) or low (vanishing) values of the gradient somewhere in the back propagation, making it hard to learn the parameters for low layers. 

\vspace{0.1in}

Consider using $\tanh$ as the activation function at each layer. Then, the gradient is close to $0$ for very large or very negative values, which is quite likely to happen somewhere in a network with many layers, so multiplying these small gradients together in back-propagation will make the contributions of the beginning of the sequence to the loss very small. Thus, it could take prohibitively long to learn weights for the beginning of the sequence. This problem is known as the problem of $\textit{vanishing gradients}$.

\subsubsection{Main idea/trick/hack for vanishing gradients}

In order to deal with vanishing gradients, we want to try to pass on more info from low layers while taking gradients, so we add connections variously called:
\begin{itemize}
    \item residual connections
    \item gated connections
    \item highway connections
    \item adaptive connections
\end{itemize}

The idea of residual connections is that we let $$\bold{h_t} = \bold{h_{t-1}} + \tanh\left(\bold{w}^{(1)} \bold{x_t} + \bold{w}^{(2)} \bold{h_{t-1}} + b\right) $$ so that taking the gradient at layer $t$ we get more information passed on from the the linear term $\bold{h}_{t-1}$ outside of the $\tanh$.

In fact, we can adaptively learn how much the gradient at each time step should be taken from the previous time step directly from the cata. Thus, we weight the contributions of the $\bold{h_t}$ and $\tanh\left(\bold{w}^{(2)} \bold{h_{t-1}} +...\right)$ by a factor $\lambda$ that is also learned from the data:
$$\bold{h_t} = \lambda \odot \bold{h}_{t-1} + (1-\lambda) \odot \tanh\left(\bold{w}^{(1)} \bold{x_t} + \bold{w}^{(2)}  \bold{h}_{t-1} + b\right)$$ 

$$\lambda = \sigma\left(\bold{w}^{(4)}h_{t-1} + \bold{w}^{(3)} \bold{x}_t + b \right)$$

By passing on information directly from previous timesteps, we can prevent vanishing gradients, since the linear terms pass on more information from previous timesteps. In this sense, the $\lambda$s function as ``memory'' of the previous timesteps. Important RNN variants using this idea are:
\begin{itemize}
    \item LSTSM (Long short-term memory networks)
    \item GRU
    \item ResNet
\end{itemize}

\subsection{Using RNNs}
\subsubsection{Classification}
Our classification algorithm has three stages:
\begin{enumerate}
    \item Run LSTM
    \item Compute Softmax   
    \item Find maximizing class
\end{enumerate}


This means that in order to make a prediction, we only need to compute 
$$ p(y_i|x_{1:T}) = \text{Softmax} (\bold{w}^{(2)} \bold{h}_i) $$
and multiply across each $y_i$ to obtain
$$ p(y_{1:T} | x_{1:T}) = \prod_{i=1}^T p(y_i|x_{1:T}) = \prod_{i=1}^T \text{Softmax} (\bold{w}^{(2)} \bold{h}_i) $$ 
Critically, this means that we do not attempt to model the relationship between the $y_i$ at all, and thus we do not need to assume any distribution over y!
\\ \\ \noindent
Lets compare this to our alternative approach, which requires full generation. Imagine that any $y_i$ is conditional on all of $y_{1:i-1}$. Then our DGM (for five nodes) is a fully connected $K_5$:

\begin{center}
\begin{tikzpicture}
  %Define nodes
  \node[latent] (y0) {$\mathbf{y_0}$};
  \node[latent, right=1cm of y0] (y1) {$\mathbf{y_1}$};
  \node[latent, right=1cm of y1] (y2) {$\mathbf{y_2}$};
  \node[latent, right=1cm of y2] (y3) {$\mathbf{y_3}$};
  \node[latent, right=1cm of y3] (y4) {$\mathbf{y_4}$};
  
  \edge{y0}{y1}
  \edge{y1}{y2}
  \edge{y2}{y3}
  \edge{y3}{y4}
  \draw [->] (y0) to [out=30,in=150] (y2);
  \draw [->] (y1) to [out=30,in=150] (y3);
  \draw [->] (y2) to [out=30,in=150] (y4);
  \draw [->] (y0) to [out=40,in=140] (y3);
  \draw [->] (y1) to [out=40,in=140] (y4);
  \draw [->] (y0) to [out=50,in=130] (y4);

\end{tikzpicture}
\end{center}

How can we then compute $p(y_s = v)$? A naive approach would be to literally enumerate all possible sequences and sum across all possibilities. However, this is very computationally expensive (exponential in $T$). Instead, we can speed up this approach by employing a greedy search. Let $$\hat{y}_1 = \text{argmax}_v \: p(y_1=v)$$
Now for each subsequent point $y_i$, we can compute
$$ \hat{y}_i = \text{argmax}_v \: p(y_1=v|\hat{y}_{i-1})$$
which is now linear in $T$, instead of exponential in $T$.

\subsubsection{Applications}
RNNs are commonly used for machine language translation and speech recognition. A simple machine translation model is that of the Encoder-Decoder model. 

\begin{center}
\begin{tikzpicture}
  %Define nodes
  \node[latent] (y1) {$\mathbf{x_1}$};
  \node[latent, right=1cm of y1] (y2) {$\mathbf{x_2}$};
  \node[latent, right=1cm of y2] (y3) {$\mathbf{x_3}$};
  \node[latent, above=1cm of y1] (h1) {$\mathbf{h_1}$};
  \node[latent, above=1cm of y2] (h2) {$\mathbf{h_2}$};
  \node[latent, above=1cm of y3] (h3) {$\mathbf{h_3}$};
  \node[latent, right=1cm of h3] (C) {$\mathbf{C}$};
  \node[latent, right=1cm of C] (h4) {$\mathbf{h_4}$};
  \node[latent, right=1cm of h4] (h5) {$\mathbf{h_5}$};
  \node[latent, right=1cm of h5] (h6) {$\mathbf{h_6}$};
  \node[latent, above=1cm of h4] (x1) {$\mathbf{y_1}$};
  \node[latent, right=1cm of x1] (x2) {$\mathbf{y_1}$};
  \node[latent, right=1cm of x2] (x3) {$\mathbf{y_2}$};
  
  \edge{y1}{h1};
  \edge{y2, h1}{h2};
  \edge{y3, h2}{h3};
  \edge{h3}{C};
  \edge{C}{h4};
  \edge{h4}{h5};
  \edge{h5}{h6};
  \edge{h4, C}{x1};
  \edge{h5, C, x1}{x2};
  \edge{h6, C, x2}{x3};

  \plate {enc} {(y1) (y2) (y3) (h1) (h2) (h3) (C)} {Encoder};
  \plate {dec} {(x1) (x2) (x3) (h4) (h5) (h6) (C)}  {Decoder};

\end{tikzpicture}
\end{center}

In this model, a sentence from the first language is fed in word by word (each $x_i$) into the Encoder. This then runs through a normal RNN setup before being fed into C, which stores the final result of the encoder. 
\\ \\ \noindent 
Now in the decoder, another RNN is run in reverse that spits out words in the second, translated, language. Each translated word  $y_i$ depends on the current layer in the decoder RNN, $h_i$, C and the last translated word (to prevent the same word from being generated multiple times). 
\begin{remark}
We will talk about \emph{Information Theory} in the next lecture.

\subsection{Practical Exercise: Speech Generator}

RNNs have been used successfully as speech generators, taking in a sequence of letters (or words) and predicting subsequent letters (words). Implement and train a character-level RNN with PyTorch, and use it to sample a sentence.

\subsubsection{Solution}

The architecture of the RNN can be conveniently implemented with torch.nn. As our training data, we downloaded all of Donald Trump's 2016 campaign speeches from \url{http://www.presidency.ucsb.edu/2016_election.php}. The concatenated data is available in \verb|concat_speeches.txt| (size: 1.3 MB). To facilitate the training process, we only included lowercase letters and a few additional symbols \verb| .:?[]| from the raw data, for a total of \verb|n_letters=32| characters. We then assembled a training set by randomly drawing 10,000 30-character sequences from the data, and used each of these to predict the next (31st) characters. We also generated a validation set by similarly drawing 500 additional character sequences. All of the input data must be converted to one-hot vectors: the entire training set was stored as an 30x10000x32 array, as required by \verb|torch.nn.LSTM| below.

We then proceeded to set up an RNN with long-short term memory (LSTM), 2 layers, and \verb|n_hidden=200| as follows:
\begin{lstlisting}[language=Python]
model = torch.nn.Sequential()
model.add_module("lstm", torch.nn.LSTM(n_letters, n_hidden, 2))
model.add_module("encoder", torch.nn.Linear(n_hidden, n_letters))
\end{lstlisting}
The linear ``encoder'' layer is necessary to convert the output of the final LSTM layer to 32 probabilities, each for a single character. A \verb|dropout| parameter can be added to the LSTM call in order to prevent overfitting (this will randomly overwrite some input nodes with zeros). We then implement the loss function via a cross-entropy layer which includes a \verb|LogSoftmax()| function:
\begin{lstlisting}[language=Python]
loss = torch.nn.CrossEntropyLoss()
\end{lstlisting}
Finally, we choose Adam from \verb|torch.optim| as our optimizer:
\begin{lstlisting}[language=Python]
optimizer = torch.optim.Adam(model.parameters(), lr=0.005)
\end{lstlisting}

The model is then trained over a significant number of epochs: it took us a few hours to get through 300 epochs on a single CPU. This performance can likely be improved by utilizing the \verb|torch.cuda| library to run the training on a GPU instead. The training function for a single epoch can be implemented as follows:
\begin{lstlisting}[language=Python]
# Train the RNN and return the loss
def train(model, optimizer, data, target):
    
    optimizer.zero_grad()
    output, hc = model.lstm(data)
    new_loss = loss(model.encoder(output[-1]), target)
    new_loss.backward()
    optimizer.step()
    return new_loss.data[0]
\end{lstlisting}
Note that we are only using the last step of the output to compare against the target: we are interested in predicting the 31st character after seeing the entire 30-character sequence. We can monitor the training and validation losses to check for convergence and overfitting.

Finally, we can feed the trained model any input sequence and let it predict the next (say, 140) characters. For example, after 175 epochs of training, simply feeding the network ``Trump:'' yields the following sentence:
\begin{lstlisting}[language=TeX,breaklines=True]
Trump: i an presisent of the sime and i wat the worlica tore and the spaising our componitien and i wat of the spared the worled the worle and the spaiting our communitien
\end{lstlisting}
This already begins to sound like real speech, considering the fact that the RNN had to learn the language from scratch! In any case, humans take years to learn to speak whereas we have only been training for a few hours. At 300 epochs, we obtained samples like this:
\begin{lstlisting}[language=TeX,breaklines=True]
Trump:ey southon fac cort the whale getton. buoller campouse tilathad sumplee ours dousting to beokew athing the semerica ince forette. [applause]
\end{lstlisting}
However, both training and validation losses were still, at that point, declining rapidly. Thus, further training will likely result in an improved performance.

This solution is merely an outline to a plausible more successful algorithm. In particular, we can expand the size of the model, the number of training sequences, or the sequence length, or tweak other relevant model parameters to train a smarter speech generator. The Python code used for this write-up is included as \verb|trump_generator.py|.

\end{remark}
\end{document}
